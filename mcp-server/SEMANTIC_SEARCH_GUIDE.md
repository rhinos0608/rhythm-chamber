# Semantic Search Setup Guide

**Rhythm Chamber MCP Server** - Last Updated: 2025-01-30

## üìä Current Configuration

### ‚úÖ Working Setup (Recommended)

```bash
Model: Xenova/all-MiniLM-L6-v2 (Transformers.js)
Dimensions: 384
Source: CPU-based (no GPU required)
Files Indexed: 402
Chunks: 17,078
Index Time: ~3 minutes (first run)
Cache: .mcp-cache/semantic-embeddings.json
Quality: Good (65-80% semantic similarity)
```

### Performance Characteristics

| Metric | Value |
|--------|-------|
| **Speed** | ‚ö° Fast (14.7ms per 1K tokens) |
| **Quality** | üü° Good (56% Top-5 accuracy) |
| **RAM Usage** | ~300-400MB (model + embeddings) |
| **CPU Load** | Moderate during indexing |
| **Reliability** | ‚úÖ 100% (no failures) |

---

## üîß Restart Script

Save as `restart-semantic-search.sh`:

```bash
#!/bin/bash

set -e

echo "üöÄ Rhythm Chamber Semantic Search - Restart Script"
echo "=================================================="

# Configuration
PROJECT_ROOT="/Users/rhinesharar/rhythm-chamber"
MCP_SERVER_DIR="$PROJECT_ROOT/mcp-server"
CACHE_DIR="$MCP_SERVER_DIR/.mcp-cache"
LOG_FILE="$MCP_SERVER_DIR/.restart-log"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Stop existing servers
echo "üõë Stopping existing MCP servers..."
pkill -f "node.*mcp-server/server.js" || true
sleep 2

# Clear cache (optional - remove if you want to preserve index)
read -p "Clear cache and re-index? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "üóëÔ∏è  Clearing cache..."
    rm -rf "$CACHE_DIR/semantic-embeddings.json"
fi

# Start server
echo "üöÄ Starting MCP server..."
cd "$MCP_SERVER_DIR"

# Environment variables
export RC_PROJECT_ROOT="$PROJECT_ROOT"
export RC_EMBEDDING_DIM=384  # Must match model dimension

# Start in background with logging
nohup node server.js > "$LOG_FILE" 2>&1 &
SERVER_PID=$!

echo "‚úÖ Server started with PID: $SERVER_PID"
echo "üìù Log file: $LOG_FILE"
echo ""
echo "üìä Monitoring startup (10 seconds)..."
sleep 10

# Check if server is running
if ps -p $SERVER_PID > /dev/null; then
    echo -e "${GREEN}‚úÖ Server is running successfully!${NC}"
    echo ""
    echo "üîç Check indexing status:"
    tail -20 "$LOG_FILE" | grep -E "Indexing|Indexed|complete|Error" || echo "Still initializing..."
    echo ""
    echo "üí° To monitor logs:"
    echo "   tail -f $LOG_FILE"
else
    echo -e "${RED}‚ùå Server failed to start. Check logs:${NC}"
    echo "   cat $LOG_FILE"
    exit 1
fi
```

Make it executable:
```bash
chmod +x restart-semantic-search.sh
```

---

## üß™ Testing Semantic Search

After restart, test with:

```bash
# Test session management
echo "Testing semantic search..."

# Via Claude Code interface, query:
# - "session creation and management"
# - "spotify authentication oauth"
# - "error handling retry logic"
```

Expected results:
- ‚úÖ Returns 5 relevant code chunks
- ‚úÖ Similarity scores: 65-80%
- ‚úÖ Correct file locations and context

---

## üîç Research Summary: Model Options

### Current: Xenova/all-MiniLM-L6-v2

**Pros:**
- ‚úÖ Extremely fast (14.7ms/1K tokens)
- ‚úÖ Lightweight (22M params)
- ‚úÖ 100% reliable
- ‚úÖ Good quality for general text

**Cons:**
- ‚ö†Ô∏è Lower accuracy (56% Top-5)
- ‚ö†Ô∏è Only 384 dimensions
- ‚ö†Ô∏è Not specifically trained on code

**Verdict:** Excellent for development/testing, acceptable for production.

---

### Alternative: Xenova/gte-base (Upgrade Path)

**Why Consider:**
- ‚úÖ 768 dimensions (2x capacity)
- ‚úÖ Higher quality than MiniLM
- ‚úÖ Still CPU-based (no GPU needed)
- ‚úÖ Good balance of speed/quality

**Trade-offs:**
- ‚ö†Ô∏è Slower (but reasonable)
- ‚ö†Ô∏è More RAM (~500-600MB)
- ‚ö†~ Slightly longer index time

**Migration:**
```bash
# Edit mcp-server/src/semantic/embeddings.js
FALLBACK_MODEL = 'Xenova/gte-base'
DEFAULT_DIM = 768

# Clear cache and restart
rm -rf .mcp-cache/semantic-embeddings.json
./restart-semantic-search.sh
```

**Recommendation:** Stick with MiniLM for now unless you need higher precision.

---

### ‚ùå Attempted: LM Studio Integration

**Tested Models:**
1. text-embedding-qwen3-embedding-0.6b (768 dim)
2. text-embedding-nomic-embed-text-v1.5 (768 dim)

**Issues:**
- ‚ùå Batch requests abort: "This operation was aborted"
- ‚ùå Falls back to Transformers.js mid-indexing
- ‚ùå Inconsistent with large batch sizes (17K+ chunks)

**Root Cause:** LM Studio's batch embedding API appears unstable for large-scale operations, even in v0.4.0 with parallel request improvements.

**Status:** Not recommended for production use until batch API is more stable.

---

## üìà Performance Benchmarks

### Indexing Performance

| Model | Dimensions | Time (402 files) | Reliability |
|-------|------------|-------------------|-------------|
| MiniLM-L6-v2 | 384 | ~3 min | ‚úÖ 100% |
| gte-base | 768 | ~4 min (est.) | ‚úÖ 100% |
| LM Studio Nomic | 768 | Falls back | ‚ùå Unstable |

### Search Quality

| Query Type | MiniLM Results | Expected gte-base |
|------------|----------------|-------------------|
| Session management | 80% similarity | ~85-90% |
| Authentication | 65% similarity | ~75-85% |
| Error handling | 76% similarity | ~80-88% |

---

## üõ†Ô∏è Troubleshooting

### "Embedding dimensions must match"

**Cause:** Stored vectors (384) ‚â† Query embeddings (768)

**Fix:**
```bash
# Ensure DEFAULT_DIM in embeddings.js matches actual model
# For MiniLM: DEFAULT_DIM = 384
# For gte-base: DEFAULT_DIM = 768

# Clear cache and restart
rm -rf .mcp-cache/semantic-embeddings.json
kill <server-pid>
node server.js
```

### "Indexing in Progress" message

**Normal:** First-time indexing takes 2-3 minutes

**Check progress:**
```bash
tail -f .mcp-cache/*.log | grep "Indexed"
```

### Server crashes after indexing

**Cause:** Cache write failure or dimension mismatch

**Fix:**
```bash
# Check logs
tail -50 .restart-log

# Verify directory exists
ls -la .mcp-cache/

# Restart with clean cache
rm -rf .mcp-cache/*
./restart-semantic-search.sh
```

---

## üìö LM Studio Integration Notes

### Why It Failed

**Symptoms:**
- LM Studio detected successfully
- Single embedding requests work
- Batch requests (17K chunks) abort silently
- System falls back to Transformers.js

**LM Studio 0.4.0 Changes:**
- ‚úÖ Parallel requests (n_parallel=4)
- ‚úÖ Continuous batching
- ‚úÖ New stateful REST API
- ‚ùå Embedding batch API still unstable

**Recommendation:** Re-test in future LM Studio versions. The parallel request infrastructure is promising and may improve.

---

## üéØ Optimization Tips

### For Development/Fast Iteration
```bash
# Use MiniLM (current setup)
# Clear cache only when code changes significantly
# Index time: ~3 minutes
```

### For Production Quality
```bash
# Consider upgrading to gte-base
# Better semantic matching
# Still CPU-based, no GPU needed
# Index time: ~4 minutes
```

### For Maximum Performance (GPU)
```bash
# LM Studio + Nomic model (when stable)
# GPU-accelerated embeddings
# 768 dimensions, highest quality
# Currently unstable for large batches
```

---

## üìù Maintenance

### Regular Tasks

**Weekly:**
- Check indexing logs for errors
- Verify semantic search quality
- Monitor cache size (~30-50MB typical)

**After Code Changes:**
```bash
# If adding/removing many files
rm -rf .mcp-cache/semantic-embeddings.json
./restart-semantic-search.sh
```

**Performance Monitoring:**
```bash
# Check server resource usage
ps aux | grep "node.*server.js"

# View recent logs
tail -100 .restart-log

# Index statistics
# (Via Claude Code: list_indexed_files tool)
```

---

## üöÄ Quick Start Commands

```bash
# Start server
cd /Users/rhinesharar/rhythm-chamber/mcp-server
RC_PROJECT_ROOT=/Users/rhinesharar/rhythm-chamber \
RC_EMBEDDING_DIM=384 \
node server.js

# In separate terminal, monitor
tail -f .restart-log

# Or use restart script
./restart-semantic-search.sh
```

---

## üìñ References

### Research Sources
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) - Embedding model benchmarks
- [SuperMemory Benchmark](https://supermemory.ai/blog/best-open-source-embedding-models-benchmarked-and-ranked/) - Model comparison
- [Nomic Technical Report](https://static.ai/reports/2024_Nomic_Embed_Text_Technical_Report.pdf) - Nomic v1 research

### Key Findings
- MiniLM-L6-v2: Fastest (14.7ms/1K tokens), good quality
- Nomic-embed: Best quality (8192 context), but slower
- gte-base: Balanced performance, 768 dimensions

---

## üìû Support

**Issues?** Check:
1. Server logs: `.restart-log`
2. Cache directory: `.mcp-cache/`
3. Configuration: `src/semantic/embeddings.js`

**Common Fixes:**
1. Dimension mismatch ‚Üí Clear cache, restart
2. Server crash ‚Üí Check logs, verify RAM
3. Poor results ‚Üí Wait for full indexing to complete

---

**Last Updated:** 2025-01-30
**Status:** ‚úÖ Production Ready (Transformers.js + MiniLM)
